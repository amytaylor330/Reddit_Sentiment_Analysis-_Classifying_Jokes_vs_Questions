{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b. Data Collection and Storage - from ELI5 subreddit\n",
    "\n",
    "This notebook is the second of four from the Reddit API scrape and classification project. \n",
    "The list of actions are performed in three sections:\n",
    " 1. **Data scrape:** `requests.get` used to download >1500 unique posts from the ELI5 (Explain Like I'm 5) subreddit\n",
    " 2. **.json data converted to dataframe:** Only relevant information from the json file was converted to a dataframe\n",
    " 3. **Data archived:** Dataframe saved as a csv. file and used in notebook #3\n",
    "\n",
    "\n",
    "A total of 1521 posts were obtained from the following two links (.json added to html for scrape):\n",
    "   - https://www.reddit.com/r/explainlikeimfive\n",
    "   - https://www.reddit.com/r/explainlikeimfive/controversial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1. Data scrape\n",
    ">**Run the next cell the first time only. COMMENT OUT AFTER RUNNING, DONT RUN AGAIN**\n",
    "<br>This is because we will run the for loop 2X. \n",
    "   - Each subreddit link (top vs. hot. vs. new vs. controversial, etc) can only download a maximum of 1000 posts because that is all that is stored in the API. \n",
    "   - We will use the for loop with one dad jokes link (top) to download up to 1000 posts.\n",
    "   - We will then use the for loop again to download an additional 1000 posts from another dad jokes link (new).\n",
    "   - This will be continued until a sufficient amount of posts have been obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # posts is an empty list our json file will be stored in, adding 25 posts a time during the for loop\n",
    "# # if after = None, the download will start from the begining of the APIs json file\n",
    "\n",
    "# posts = []\n",
    "# after = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Instructions for code block below:__ \n",
    "   1. Set `url = url_1`\n",
    "   2. Add a `user_agent` name to the header. The name is arbitrary. \n",
    "   3. Run the for loop to submit 1000 requests. Each request will contain a max of 25 posts. The total number of unique posts that can be downloaded per link is 1000.\n",
    "   4. Check the number of unique posts.\n",
    "   5. Repeat step #1 with a new link (set `url = url_2`); repeat steps 2-4 until >1500 unique posts are obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1 = 'https://www.reddit.com/r/explainlikeimfive.json'\n",
    "url_2 = 'https://www.reddit.com/r/explainlikeimfive/controversial/.json'\n",
    "# url_3 = 'https://www.reddit.com/r/explainlikeimfive/top/.json'\n",
    "# url_4 = 'https://www.reddit.com/r/explainlikeimfive/rising/.json'\n",
    "\n",
    "url = url_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change header to collect more posts\n",
    "header = {\"user_agent\": \"sarahruggles452\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    print(i)\n",
    "    if after == None:\n",
    "        params = {}\n",
    "    else: \n",
    "        params = {'after': after}\n",
    "    params = {'after': after}\n",
    "    res = requests.get(url, params = params, headers = header)\n",
    "    if res.status_code == 200:\n",
    "        the_json = res.json()\n",
    "        posts.extend(the_json['data']['children'])\n",
    "        after = the_json['data']['after']\n",
    "\n",
    "    time.sleep(0.4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of total posts downloaded and unique posts\n",
    "print(len(posts))\n",
    "len(set([p['data']['name'] for p in posts]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So out of 12964 posts, 1521 are unique. This is enough to perform my classification analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 2: Convert json to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [x['data'] for x in posts]\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine the column names, decide which columns to convert to df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop duplicate posts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List of columns to keep:**\n",
    "    'name', 'title', 'selftext', \n",
    "    'subreddit', 'created', 'author',\n",
    "    'num_comments', 'ups', 'downs', 'score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame from the name column\n",
    "df = [p['data']['name'] for p in posts]\n",
    "df = pd.DataFrame(df, columns= ['name'])\n",
    "\n",
    "# add REQUIRED columns\n",
    "df['title'] = [p['data']['title'] for p in posts]\n",
    "df['selftext'] = [p['data']['selftext'] for p in posts]\n",
    "\n",
    "# add ADDITIONAL columns (just for fun)\n",
    "df['subreddit'] = [p['data']['subreddit'] for p in posts]\n",
    "df['created'] = [p['data']['created'] for p in posts]\n",
    "df['author'] = [p['data']['author'] for p in posts]\n",
    "df['num_comments'] = [p['data']['num_comments'] for p in posts]\n",
    "df['ups'] = [p['data']['ups'] for p in posts]\n",
    "df['downs'] = [p['data']['downs'] for p in posts]\n",
    "df['score'] = [p['data']['score'] for p in posts]\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: **Save dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: REMEMBER TO COMMENT OUT AFTER EXECUTING. index=False makes df save without adding another index\n",
    "# df.to_csv(\"./datasets/eli5.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
