{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a. Data Collection and Storage - from Dad Jokes subreddit\n",
    "\n",
    "This notebook is the first of four from the Reddit API scrape and classification project. \n",
    "The list of actions are performed in three sections:\n",
    " 1. **Data scrape:** `requests.get` used to download >1500 unique posts from the Dad Jokes subreddit\n",
    " 2. **.json data converted to dataframe:** Only relevant information from the json file was converted to a dataframe\n",
    " 3. **Data archived:** Dataframe saved as a csv. file and used in notebook #3\n",
    "\n",
    "\n",
    "A total of 1738 posts were obtained from the following two links (.json added to html for scrape):\n",
    "   - https://www.reddit.com/r/dadjokes/top/?t=month\n",
    "   - https://www.reddit.com/r/dadjokes/new/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1. Data scrape\n",
    ">**Run the next cell the first time only. COMMENT OUT AFTER RUNNING, DONT RUN AGAIN**\n",
    "<br>This is because we will run the for loop 2X. \n",
    "   - Each subreddit link (top vs. hot. vs. new vs. controversial, etc) can only download a maximum of 1000 posts because that is all that is stored in the API. \n",
    "   - We will use the for loop with one dad jokes link (top) to download up to 1000 posts.\n",
    "   - We will then use the for loop again to download an additional 1000 posts from another dad jokes link (new).\n",
    "   - This will be continued until a sufficient amount of posts have been obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts is an empty list our json file will be stored in, adding 25 posts a time during the for loop\n",
    "# if after = None, the download will start from the begining of the APIs json file\n",
    "\n",
    "# posts = []\n",
    "# after = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Instructions for code blocks below:__ \n",
    "   1. Set `url = url_1`\n",
    "   2. Add a `user_agent` name to the header. The name is arbitrary. \n",
    "   3. Run the for loop to submit 1000 requests. Each request will contain a max of 25 posts. The total number of unique posts that can be downloaded per link is 1000.\n",
    "   4. Check the number of unique posts.\n",
    "   5. Repeat step #1 with a new link (set `url = url_2`); repeat steps 2-4 until >1500 unique posts are obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1 = \"https://www.reddit.com/r/dadjokes/top.json?t=month\"\n",
    "url_2 = \"https://www.reddit.com/r/dadjokes/new/.json\"\n",
    "#url_3 = 'https://www.reddit.com/r/dadjokes/hot/.json'\n",
    "\n",
    "url = url_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change header to collect more posts. change username if necessary\n",
    "header = {\"user_agent\": \"amytaylor\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    print(i)\n",
    "    if after == None:\n",
    "        params = {}\n",
    "    else: \n",
    "        params = {'after': after}\n",
    "    params = {'after': after}\n",
    "    res = requests.get(url, params = params, headers = header)\n",
    "    if res.status_code == 200:\n",
    "        the_json = res.json()\n",
    "        posts.extend(the_json['data']['children'])\n",
    "        after = the_json['data']['after']\n",
    "\n",
    "    time.sleep(0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1738"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the length of total posts downloaded and unique posts\n",
    "print(len(posts))\n",
    "len(set([p['data']['name'] for p in posts]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So out of 3447 posts, 1738 are unique. This is enough to perform my classification analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 2: Convert json to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [x['data'] for x in posts]\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3447, 99)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine the column names, decide which columns to convert to df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['approved_at_utc', 'approved_by', 'archived', 'author',\n",
       "       'author_cakeday', 'author_flair_background_color',\n",
       "       'author_flair_css_class', 'author_flair_richtext',\n",
       "       'author_flair_template_id', 'author_flair_text',\n",
       "       'author_flair_text_color', 'author_flair_type', 'author_fullname',\n",
       "       'author_patreon_flair', 'banned_at_utc', 'banned_by', 'can_gild',\n",
       "       'can_mod_post', 'category', 'clicked', 'content_categories',\n",
       "       'contest_mode', 'created', 'created_utc', 'crosspost_parent',\n",
       "       'crosspost_parent_list', 'distinguished', 'domain', 'downs', 'edited',\n",
       "       'gilded', 'gildings', 'hidden', 'hide_score', 'id', 'is_crosspostable',\n",
       "       'is_meta', 'is_original_content', 'is_reddit_media_domain',\n",
       "       'is_robot_indexable', 'is_self', 'is_video', 'likes',\n",
       "       'link_flair_background_color', 'link_flair_css_class',\n",
       "       'link_flair_richtext', 'link_flair_template_id', 'link_flair_text',\n",
       "       'link_flair_text_color', 'link_flair_type', 'locked', 'media',\n",
       "       'media_embed', 'media_only', 'mod_note', 'mod_reason_by',\n",
       "       'mod_reason_title', 'mod_reports', 'name', 'no_follow', 'num_comments',\n",
       "       'num_crossposts', 'num_reports', 'over_18', 'parent_whitelist_status',\n",
       "       'permalink', 'pinned', 'post_hint', 'preview', 'pwls', 'quarantine',\n",
       "       'removal_reason', 'report_reasons', 'saved', 'score', 'secure_media',\n",
       "       'secure_media_embed', 'selftext', 'selftext_html', 'send_replies',\n",
       "       'spoiler', 'stickied', 'subreddit', 'subreddit_id',\n",
       "       'subreddit_name_prefixed', 'subreddit_subscribers', 'subreddit_type',\n",
       "       'suggested_sort', 'thumbnail', 'thumbnail_height', 'thumbnail_width',\n",
       "       'title', 'ups', 'url', 'user_reports', 'view_count', 'visited',\n",
       "       'whitelist_status', 'wls'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop duplicate posts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1738, 99)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List of columns to keep:**\n",
    "    'name', 'title', 'selftext', \n",
    "    'subreddit', 'created', 'author',\n",
    "    'num_comments', 'ups', 'downs', 'score'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame from the name column\n",
    "df = [p['data']['name'] for p in posts]\n",
    "df = pd.DataFrame(df, columns= ['name'])\n",
    "\n",
    "# add REQUIRED columns\n",
    "df['title'] = [p['data']['title'] for p in posts]\n",
    "df['selftext'] = [p['data']['selftext'] for p in posts]\n",
    "\n",
    "# add ADDITIONAL columns (just for fun)\n",
    "df['subreddit'] = [p['data']['subreddit'] for p in posts]\n",
    "df['created'] = [p['data']['created'] for p in posts]\n",
    "df['author'] = [p['data']['author'] for p in posts]\n",
    "df['num_comments'] = [p['data']['num_comments'] for p in posts]\n",
    "df['ups'] = [p['data']['ups'] for p in posts]\n",
    "df['downs'] = [p['data']['downs'] for p in posts]\n",
    "df['score'] = [p['data']['score'] for p in posts]\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: **Save dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: REMEMBER TO COMMENT OUT AFTER EXECUTING. index=False makes df save without adding another index\n",
    "# df.to_csv(\"./datasets/dad.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
